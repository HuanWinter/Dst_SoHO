{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ import package\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import h5py\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from ipdb import set_trace as st\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import linecache as lc\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import io as sio\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from funs import SoHO_read, Read_solar_image, \\\n",
    "    dt2date, Prob_train\n",
    "\n",
    "\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Config ######################\n",
    "\n",
    "time_stamp = 'Data/time_data_19873_train_valid_test.h5'\n",
    "channels = ['MDI','EIT','LASCO'] # , ,'LASCO_diff','MDI_diff',\n",
    "\n",
    "dst_peak = -100\n",
    "delay = 24\n",
    "time_res = 2\n",
    "Peak_width = 5\n",
    "Peak_dis = 120\n",
    "F107_thres = [0, 500]\n",
    "SDO_flag = 0\n",
    "storm_idx = [15]\n",
    "var_idx = [0, 1, 2]\n",
    "\n",
    "delay_range = 12*time_res\n",
    "delay_hour_clu = range(delay, delay+delay_range, time_res)\n",
    "SoHO_file = 'Res/Solar_data_19873.h5'\n",
    "Omni_data = 'Data/1999-2012.pkl'\n",
    "hmi_file = 'Data/hmi_halloween2021.h5'\n",
    "\n",
    "Res_name = 'Res/Dst_'+\\\n",
    "    str(delay)+'-'+\\\n",
    "    str(delay+delay_range) + '-'+\\\n",
    "    str(time_res) + '--'+\\\n",
    "    str(dst_peak)+'.h5'\n",
    "\n",
    "# filename_Y = 'Results/Bz_GSE_0-48.h5'\n",
    "callname = 'Res/params_'+\\\n",
    "    str(np.array(var_idx))+'_'+ \\\n",
    "    str(delay)+'-' +\\\n",
    "    str(delay+12*time_res) +'--'+\\\n",
    "    str(time_res)+'-' +\\\n",
    "    str(dst_peak)+'-'+\\\n",
    "    str(storm_idx[0])+'.pt'\n",
    "\n",
    "callname_opt = 'Res/params_opt_'+\\\n",
    "    str(np.array(var_idx))+'_'+ \\\n",
    "    str(delay)+'-' +\\\n",
    "    str(delay+12*time_res) +'--'+\\\n",
    "    str(time_res)+'-' +\\\n",
    "    str(dst_peak)+'-'+\\\n",
    "    str(storm_idx[0])+'.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value count     0/113963\n"
     ]
    }
   ],
   "source": [
    "################### global variables #################\n",
    "\n",
    "df = pd.read_pickle(Omni_data)\n",
    "omni_data = df['DST']\n",
    "omni_date = df.index\n",
    "\n",
    "# Fill missing values\n",
    "print(f'Missing value count \\\n",
    "    {omni_data.isna().sum()}/{len(omni_data)}')\n",
    "omni_data.interpolate(inplace=True)\n",
    "omni_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['test_1927_dates', 'train_15645_dates', 'valid_2301_dates']>\n"
     ]
    }
   ],
   "source": [
    "###################### SoHO data (X, run it once) ######################\n",
    "\n",
    "with h5py.File(time_stamp,'r') as f:\n",
    "    print(f.keys())\n",
    "    train_date = np.array(f['train_15645_dates'])\n",
    "    valid_date = np.array(f['valid_2301_dates'])\n",
    "    test_date = np.array(f['test_1927_dates'])\n",
    "    f.close()\n",
    "\n",
    "all_date = np.vstack([valid_date, train_date, test_date])\n",
    "X_all, Y_all = SoHO_read(channels, win_size=1)\n",
    "\n",
    "with h5py.File(SoHO_file, 'w') as f:\n",
    "\n",
    "    f.create_dataset('X', data=X_all)\n",
    "    # f.create_dataset('Y', data=Y_all)\n",
    "    f.create_dataset('date', data=all_date)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19873/19873 [02:53<00:00, 114.34it/s]\n"
     ]
    }
   ],
   "source": [
    "###################### Dst data (Y, run it once) ######################\n",
    "\n",
    "with h5py.File(SoHO_file, 'r+') as f:\n",
    "\n",
    "    for v in ['Y']:\n",
    "        if v in f:\n",
    "            del f[v]\n",
    "    \n",
    "    all_date = np.array(f['date'])\n",
    "\n",
    "    out = np.zeros([len(all_date), delay_range//time_res])\n",
    "\n",
    "    for idx in tqdm(range(len(all_date))):\n",
    "        out[idx] = Read_solar_image(idx, omni_data, \n",
    "                                    omni_date, all_date,\n",
    "                                    time_res, delay_range, delay)\n",
    "\n",
    "    f.create_dataset('Y', data=out)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########dst storm time selection(run it once) ###################\n",
    "\n",
    "dates = []\n",
    "sample_storm = []\n",
    "n = 1\n",
    "\n",
    "with h5py.File(SoHO_file, 'r') as f:\n",
    "    X = np.array(f['X'])\n",
    "    Y = np.array(f['Y'])\n",
    "    all_date = np.array(f['date'])\n",
    "    f.close()\n",
    "\n",
    "if SDO_flag:\n",
    "    with h5py.File(hmi_file, 'r') as f:\n",
    "\n",
    "        X_ex = np.array(f['data'])\n",
    "        Y_reg_ex = np.array(f['y'])\n",
    "        Y_ex = np.zeros(Y_reg_ex.shape)\n",
    "        Y_ex[Y_reg_ex<dst_peak] = 1\n",
    "        date_ex = np.array(f['date'])\n",
    "        f.close()\n",
    "\n",
    "for ind in tqdm(range(len(all_date))):\n",
    "    date = dt2date(all_date[ind], time_res)\n",
    "    dates.append(date)\n",
    "\n",
    "peaks, _ = find_peaks(omni_data*-1,\n",
    "                    # height=np.abs(args.Dst_sel),\n",
    "                    distance=Peak_dis,\n",
    "                    width=Peak_width)\n",
    "idx = np.where(omni_data[peaks] <= dst_peak+20)[0]\n",
    "\n",
    "idx_clu = np.zeros([len(idx), 2])\n",
    "\n",
    "for i, idx_t in tqdm(enumerate(idx)):\n",
    "\n",
    "    print('peak {}:'.format(i), omni_date[peaks[idx_t]])\n",
    "\n",
    "    idx_clu[i, 0] = np.where(\n",
    "        omni_data[:peaks[idx_t]] >= 0)[0][-1]-delay_range\n",
    "    idx_clu[i, 1] = np.where(\n",
    "        omni_data[peaks[idx_t]:] >= 0)[0][0]+delay_range+peaks[idx_t]\n",
    "\n",
    "idx_clu = idx_clu.astype(int)\n",
    "\n",
    "for i, idx in tqdm(enumerate(idx_clu)):\n",
    "    date_end = omni_date[int(idx[1])]\n",
    "    date_beg = omni_date[int(idx[0])]\n",
    "\n",
    "    index_image = [j for j in range(len(dates)) if\n",
    "                    ((date_beg <= dates[j]+dt.timedelta(hours=delay))\n",
    "                    & (date_end >= dates[j]+dt.timedelta(hours=delay)))\n",
    "                    ]\n",
    "\n",
    "    if len(index_image) >= 28:\n",
    "        gap = np.zeros(len(index_image) - 1)\n",
    "        for k, idx_image in enumerate(index_image[:-1]):\n",
    "            gap_t = dates[index_image[k+1]] - dates[index_image[k]]\n",
    "            gap[k] = gap_t.seconds//3600\n",
    "\n",
    "        if ((gap > time_res*3).sum() <= 15) & (len(index_image) > 30):\n",
    "\n",
    "            sample_storm.append(index_image)\n",
    "            print('size of {}th storm should be {}/{}'\\\n",
    "                .format(n, (idx[1]-idx[0])//time_res, \n",
    "                len(index_image)))\n",
    "            \n",
    "            # date_plot = [dates[j] for j in index_image]\n",
    "            # plt.plot(date_plot, \\\n",
    "            #     Y[index_image, 0], \n",
    "            #     'r.')\n",
    "            # plt.xticks(rotation='vertical')\n",
    "\n",
    "            # plt.savefig('Figs/sample_'+str(n)+'.jpg', dpi=300)\n",
    "            # plt.close()\n",
    "            print('start/end time {}/{}'.format(date_beg, date_end))\n",
    "            # print('end time', date_end)\n",
    "            \n",
    "            n += 1\n",
    "\n",
    "            if n == 51:\n",
    "                break\n",
    "\n",
    "with h5py.File(Res_name, 'w') as f:\n",
    "\n",
    "    for v in ['X_ex', 'Y_ex', 'Y_reg_ex', \\\n",
    "        'time_ex', 'storm_num']:\n",
    "        if v in f:\n",
    "            del f[v]\n",
    "    # import ipdb;ipdb.set_trace()\n",
    "    for i in tqdm(range(len(sample_storm))):\n",
    "\n",
    "        idx = sample_storm[i]\n",
    "        for v in ['X_'+str(i), 'Y_'+str(i),\n",
    "                    'Y_reg_'+str(i), 'time_'+str(i)]:\n",
    "            if v in f:\n",
    "                del f[v]\n",
    "\n",
    "        f.create_dataset('X_'+str(i), data=X[idx])\n",
    "        f.create_dataset('Y_reg_'+str(i), data=Y[idx])\n",
    "        f.create_dataset('Y_'+str(i), data=Y[idx]<dst_peak)\n",
    "        f.create_dataset('time_'+str(i), data=all_date[idx])\n",
    "    f.create_dataset('X_ex', data=X_ex)\n",
    "    f.create_dataset('time_ex', data=date_ex)\n",
    "    f.create_dataset('Y_ex', data=Y_ex)\n",
    "    f.create_dataset('Y_reg_ex', data=Y_reg_ex)\n",
    "    f.create_dataset('storm_num', data=len(sample_storm))\n",
    "\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd40fc67c7a1d91df4ca13919301e6ab29b05d53552f9fb4893f522d4c0bcec3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('pwork')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
